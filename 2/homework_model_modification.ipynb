{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b169119-7ca3-4a98-a1ef-cac884312c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split #для разделения на тренировочные и тестовые выборки\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def make_regression_data(n=100, noise=0.2, source='random'):\n",
    "    if source== 'random':\n",
    "        X = torch.randn(n,1)\n",
    "        w,b = -5, 10\n",
    "        y = w*X + b + noise * torch.randn(n, 1)\n",
    "        return X,y\n",
    "    \n",
    "    elif source == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        data = load_diabetes()\n",
    "        X = torch.tensor(data['data'], dtype=torch.float32)\n",
    "        y = torch.tensor(data['target'], dtype=torch.float32).unsqueeze(1)\n",
    "        return X, y\n",
    "    else:\n",
    "        raise ValueError('Unk sources')\n",
    "        \n",
    "def mse(y_pred: torch.Tensor, y_true:torch.Tensor) -> torch.Tensor:\n",
    "    return ((y_pred - y_true)**2).mean()\n",
    "        \n",
    "def log_epoch(epoch, avg_loss, **metrics):\n",
    "    message = f'Epoch: {epoch}\\t loss:{avg_loss}'\n",
    "    for k, v in metrics.items():\n",
    "        message += f'\\t{k}: {v:.4f}'\n",
    "    print(message)\n",
    "    \n",
    "def make_classification_data(n = 100):\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    data = load_breast_cancer()\n",
    "    X = torch.tensor(data['data'], dtype=torch.float32)\n",
    "    y = torch.tensor(data['target'], dtype=torch.float32).unsqueeze(1)\n",
    "    return X, y\n",
    "\n",
    "def make_multiclass_data(): #датасет с множеством классов\n",
    "    from sklearn.datasets import load_wine\n",
    "    data = load_wine()\n",
    "    X = torch.tensor(data.data, dtype=torch.float32)\n",
    "    y = torch.tensor(data.target, dtype=torch.long) \n",
    "    return X, y\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_bin = (y_pred>0.5).float()\n",
    "    return (y_pred_bin == y_true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59612b3d-0958-4e00-bc8a-f2a7909f9d28",
   "metadata": {},
   "source": [
    "# №1.1 LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1fd1e9c6-8754-4dc7-add0-e511e2bcca80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear.forward(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ca76553-fe29-4313-9371-71a509d9891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8982]]) tensor([9.9774])\n",
      "tensor([[-4.9291]]) tensor([10.0438])\n",
      "tensor([[-4.9449]]) tensor([10.0008])\n",
      "tensor([[-4.9556]]) tensor([9.9876])\n",
      "tensor([[-4.9473]]) tensor([9.9416])\n",
      "tensor([[-4.9378]]) tensor([10.0318])\n",
      "tensor([[-4.9459]]) tensor([9.9775])\n",
      "tensor([[-4.9077]]) tensor([10.0229])\n",
      "tensor([[-4.9153]]) tensor([9.9769])\n",
      "Epoch: 10\t loss:0.03885782801080495\n",
      "tensor([[-4.9569]]) tensor([10.0150])\n",
      "tensor([[-4.9695]]) tensor([9.9823])\n",
      "tensor([[-4.9543]]) tensor([10.0079])\n",
      "tensor([[-4.9195]]) tensor([10.0269])\n",
      "tensor([[-5.0076]]) tensor([9.9923])\n",
      "tensor([[-4.9454]]) tensor([10.0006])\n",
      "tensor([[-4.9591]]) tensor([10.0147])\n",
      "tensor([[-4.9189]]) tensor([9.9636])\n",
      "tensor([[-4.9344]]) tensor([9.9801])\n",
      "tensor([[-4.9974]]) tensor([9.9804])\n",
      "Epoch: 20\t loss:0.043882836354896426\n",
      "tensor([[-4.9143]]) tensor([9.9965])\n",
      "tensor([[-4.9712]]) tensor([9.9653])\n",
      "tensor([[-4.9756]]) tensor([10.0291])\n",
      "tensor([[-4.9594]]) tensor([10.0016])\n",
      "tensor([[-4.9475]]) tensor([10.0185])\n",
      "tensor([[-4.9409]]) tensor([9.9791])\n",
      "tensor([[-4.9082]]) tensor([9.9697])\n",
      "tensor([[-4.9100]]) tensor([9.9768])\n",
      "tensor([[-4.9649]]) tensor([9.9932])\n",
      "tensor([[-4.9474]]) tensor([10.0283])\n",
      "Epoch: 30\t loss:0.04029263462871313\n",
      "tensor([[-4.9473]]) tensor([10.0247])\n",
      "tensor([[-4.9560]]) tensor([10.0051])\n",
      "tensor([[-4.9445]]) tensor([10.0020])\n",
      "tensor([[-4.9133]]) tensor([10.0141])\n",
      "tensor([[-4.9682]]) tensor([10.0062])\n",
      "tensor([[-4.9195]]) tensor([9.9846])\n",
      "tensor([[-4.9657]]) tensor([10.0048])\n",
      "tensor([[-4.9798]]) tensor([9.9763])\n",
      "tensor([[-4.9614]]) tensor([10.0098])\n",
      "tensor([[-4.9248]]) tensor([10.0592])\n",
      "Epoch: 40\t loss:0.03698375774547458\n",
      "Early stopping на эпохе - 40\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression_data(10000)\n",
    "EPOCHS = 100\n",
    "dataset = CustomDataset(X, y)\n",
    "\n",
    "# Решил разделить датасет на тренировочную и тестовую выборки, так как по определению early_stop ориентируется на ошибку \n",
    "# валидационной выборки\n",
    "train_size = int(0.8 * len(dataset)) # train\n",
    "val_size = int(0.2 * len(dataset)) # validate\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size = 128,\n",
    "#     shuffle = True,\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "L1_coefficient = 0.01 # Добавили коэфициент влияния штрафа на Лассо\n",
    "L2_coefficient = 0.01 # Добавили коэфициент влияния штрафа на Ridge \n",
    "Lasso = 0\n",
    "Ridge = 0\n",
    "\n",
    "early_stop = EPOCHS*0.4 # этот параметр нужен, чтобы иметь лимит при обучении, в случае увеличения эпох, качество ошибки не меняется или ниже\n",
    "no_improvement = 0\n",
    "best_val_loss = 1e-9\n",
    "delta_number = 0.0001 # Добавил, чтобы модель учитывала не все улучшения, а только значимые. А то 1^(-10) улучшение как то несерьезно\n",
    "\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 100\n",
    "model = LinearRegressionTorch(1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(batch_x)\n",
    "        \n",
    "        \n",
    "        if L1_coefficient != 0 or L2_coefficient != 0:    \n",
    "            Lasso = 0\n",
    "            Ridge = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'bias' not in name: # в формуле Лассо и Ridge мы суммируем только веса\n",
    "                    # Ошибка по формуле sum(|wi|) * coef, а после param.abs ставим .sum, так как param - тензор\n",
    "                    Lasso += param.abs().sum() \n",
    "                    # Ridge это то же Лассо, только веса беруться в квадрате\n",
    "                    Ridge += (param ** 2).sum()\n",
    "                    \n",
    "        '''\n",
    "        Сделал универсальный способ учета ошибки.\n",
    "        Теперь это и LassoRegression в случае если будет указан только L1, и RidgeRegression если указывать только L2, \n",
    "        и ElasticNetRegression если указан и L1 и L2, и обычная линейная регрессия если ничего не указывать \n",
    "        \n",
    "        Все по формуле ElasticNet = MSE + sum(|wi|) * coef_l1 + sum(wi^2) * coef_l2 \n",
    "        '''\n",
    "        loss = loss_fn.forward(y_pred, batch_y) + L1_coefficient * Lasso + L2_coefficient * Ridge\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader) #ошибка на обучающей выборке \n",
    "\n",
    "    model.eval() #переключаем модель в режим оценки \n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            y_pred = model(batch_x)\n",
    "            loss = loss_fn(y_pred, batch_y) # Здесь штрафы не нужны, так как мы проверям качество модели в чистом виде\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    if avg_val_loss < best_val_loss-delta_number:\n",
    "        no_improvement = 0\n",
    "        best_val_loss = avg_val_loss\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        log_epoch(epoch, avg_val_loss)\n",
    " \n",
    "    if early_stop <= no_improvement:\n",
    "        print(f\"Early stopping на эпохе - {epoch}\")\n",
    "        break\n",
    "    print(model.linear.weight.data, model.linear.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0248663-6106-48a0-8c27-76feff1a7962",
   "metadata": {},
   "source": [
    "# №1.2 LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48551048-e469-464c-81c3-f24b9151b9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, in_features, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear.forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "20de415b-2390-42f2-a294-f28376f93871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Сделаю только для бинарной классификации, сложно как то для 1.5 часа решения домашки\n",
    "def accuracy(y_pred, y_true): #вынес в отдельную функцию\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float()\n",
    "    return (y_pred == y_true).float().mean().item()\n",
    "\n",
    "def precision(y_pred, y_true):\n",
    "    y_pred = (y_pred.sigmoid() > 0.5).float()#Применяем sigmoid, чтобы получить вероятности от 0 до 1\n",
    "    #это случаи (tp), когда модель сказала да (y_pred == 1) и на самом деле - да (y_true == 1) \n",
    "    tp = (y_pred * y_true).sum().item()\n",
    "    #это случаи (fp), когда модель сказала да (y_pred == 1), а на самом деле - нет (y_true == 0).\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum().item()\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y_pred, y_true):\n",
    "    y_pred = (y_pred.sigmoid() > 0.5).float()\n",
    "    tp = (y_pred * y_true).sum().item()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum().item()\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(y_pred, y_true):\n",
    "    p = precision(y_pred, y_true)\n",
    "    r = recall(y_pred, y_true)\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2*p*r / (p + r) #изначально по формуле 2/(1/p + 1/r), но я раскрыл скобки просто\n",
    "\n",
    "def roc_auc(y_pred, y_true):\n",
    "    y_scores = y_pred.sigmoid().detach().numpy()#выход модели до sigmoid\n",
    "    y_true = y_true.numpy()#реальные метки [0, 1]\n",
    "    \n",
    "    #cортировка по уверенности модели\n",
    "    #мы делаем список пар (score, true_label) и сортируем его по убыванию уверенности модели.\n",
    "    y_scores = y_scores.reshape(-1)  #делаем 1d (на случай, если пришёл 2d)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    scores = np.column_stack((y_scores, y_true))  # связываем предсказания и правду\n",
    "    scores = scores[scores[:, 0].argsort()][::-1]  # сортировка по убыванию уверенности\n",
    "    \n",
    "    #Общее число положительных и отрицательных примеров\n",
    "    total_pos = y_true.sum()\n",
    "    total_neg = len(y_true) - total_pos\n",
    "\n",
    "    tp = fp = 0\n",
    "    tpr_list = [0]\n",
    "    fpr_list = [0]\n",
    "\n",
    "    for score in scores:\n",
    "        if score[1] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr = tp / total_pos\n",
    "        fpr = fp / total_neg\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    #Вычисление площади под кривой (AUC)\n",
    "    auc = 0\n",
    "    for i in range(1, len(tpr_list)):\n",
    "        #Интегрирование методом трапеций\n",
    "        auc += (fpr_list[i] - fpr_list[i - 1]) * (tpr_list[i] + tpr_list[i - 1]) / 2\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y_pred, y_true):\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float().numpy().flatten()# Преобразуем в numpy для удобства\n",
    "    y_true = y_true.numpy().flatten()\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    # Выводим confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"               | Predicted Pos | Predicted Neg |\")\n",
    "    print(\"---------------|--------------|--------------|\")\n",
    "    print(f\"Actual Pos     |     {tp:4d}    |     {fn:4d}    |\")\n",
    "    print(f\"Actual Neg     |     {fp:4d}    |     {tn:4d}    |\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Считаем метрики\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Выводим метрики\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall   : {recall:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a496c141-4405-4eb4-b835-ca753797da1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 классы\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12296\\3883148379.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print_confusion_matrix(torch.tensor(all_probs), torch.tensor(all_true))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       72    |        0    |\n",
      "Actual Neg     |       39    |        3    |\n",
      "\n",
      "\n",
      "Accuracy : 0.6579\n",
      "Precision: 0.6486\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.7869\n",
      "Epoch: 10\t loss:1983.7006427447002\taccuracy: 0.6260\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       48    |       24    |\n",
      "Actual Neg     |        2    |       40    |\n",
      "\n",
      "\n",
      "Accuracy : 0.7719\n",
      "Precision: 0.9600\n",
      "Recall   : 0.6667\n",
      "F1 Score : 0.7869\n",
      "Epoch: 20\t loss:1264.7099523544312\taccuracy: 0.6632\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       50    |       22    |\n",
      "Actual Neg     |        1    |       41    |\n",
      "\n",
      "\n",
      "Accuracy : 0.7982\n",
      "Precision: 0.9804\n",
      "Recall   : 0.6944\n",
      "F1 Score : 0.8130\n",
      "Epoch: 30\t loss:1749.1208685768975\taccuracy: 0.6505\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       45    |       27    |\n",
      "Actual Neg     |        1    |       41    |\n",
      "\n",
      "\n",
      "Accuracy : 0.7544\n",
      "Precision: 0.9783\n",
      "Recall   : 0.6250\n",
      "F1 Score : 0.7627\n",
      "Epoch: 40\t loss:1098.5559719668495\taccuracy: 0.6778\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       46    |       26    |\n",
      "Actual Neg     |        1    |       41    |\n",
      "\n",
      "\n",
      "Accuracy : 0.7632\n",
      "Precision: 0.9787\n",
      "Recall   : 0.6389\n",
      "F1 Score : 0.7731\n",
      "Epoch: 50\t loss:990.1389558580187\taccuracy: 0.6982\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       46    |       26    |\n",
      "Actual Neg     |        1    |       41    |\n",
      "\n",
      "\n",
      "Accuracy : 0.7632\n",
      "Precision: 0.9787\n",
      "Recall   : 0.6389\n",
      "F1 Score : 0.7731\n",
      "Epoch: 60\t loss:908.7001145680746\taccuracy: 0.7039\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       70    |        2    |\n",
      "Actual Neg     |        6    |       36    |\n",
      "\n",
      "\n",
      "Accuracy : 0.9298\n",
      "Precision: 0.9211\n",
      "Recall   : 0.9722\n",
      "F1 Score : 0.9459\n",
      "Epoch: 70\t loss:1086.0767902798123\taccuracy: 0.6818\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       70    |        2    |\n",
      "Actual Neg     |        4    |       38    |\n",
      "\n",
      "\n",
      "Accuracy : 0.9474\n",
      "Precision: 0.9459\n",
      "Recall   : 0.9722\n",
      "F1 Score : 0.9589\n",
      "Epoch: 80\t loss:808.2126558091906\taccuracy: 0.6979\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       70    |        2    |\n",
      "Actual Neg     |        3    |       39    |\n",
      "\n",
      "\n",
      "Accuracy : 0.9561\n",
      "Precision: 0.9589\n",
      "Recall   : 0.9722\n",
      "F1 Score : 0.9655\n",
      "Epoch: 90\t loss:666.8935457865397\taccuracy: 0.7026\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       70    |        2    |\n",
      "Actual Neg     |        5    |       37    |\n",
      "\n",
      "\n",
      "Accuracy : 0.9386\n",
      "Precision: 0.9333\n",
      "Recall   : 0.9722\n",
      "F1 Score : 0.9524\n",
      "Epoch: 100\t loss:619.0000157296746\taccuracy: 0.7217\n",
      "Epoch 100 | Loss: 0.3381 | Acc: 0.9386, Prec: 0.9333, Rec: 0.9722, F1: 0.9524, AUC: 0.9289\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification_data()\n",
    "EPOCHS = 100\n",
    "dataset = CustomDataset(X, y)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 16,\n",
    "    shuffle = True,\n",
    ")\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "\n",
    "n_classes = len(torch.unique(y))# количество разнообразных классов\n",
    "print(n_classes,'классы')\n",
    "\n",
    "if n_classes > 2:\n",
    "    model = LogisticRegressionTorch(X.shape[1], n_classes)# выбираем модель с несколькими классами\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() # Взял функцию для высчитывания ошибки в случае, если берем многоклассовый датасет\n",
    "    '''\n",
    "    С многоклассовым датасетом лучше работает Adam оптимизатор, \n",
    "    чем с SGD (SGD тоже работает нормально, но может застрять в локальных минимумах)\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "else:\n",
    "    model = LogisticRegressionTorch(X.shape[1])# выбираем модель с несколькими классами\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "# Разделение на train/val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()#включаем режим обучения\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_prec = 0\n",
    "    total_rec = 0\n",
    "    total_f1 = 0\n",
    "    total_auc = 0\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(batch_x)\n",
    "        loss = loss_fn.forward(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if n_classes > 2:\n",
    "            preds = y_pred.argmax(dim=1)#Для accuracy сравниваем argmax предсказаний и реальных меток\n",
    "            total_acc += (preds == batch_y).float().mean().item()\n",
    "        else:\n",
    "            probs = torch.sigmoid(y_pred)\n",
    "            preds = (probs > 0.5).float()\n",
    "            total_acc += (preds == batch_y).float().mean().item()\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_acc/ len(dataloader)\n",
    "    \n",
    "    model.eval()#включаем для обучения\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_true = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            y_pred_logits = model(batch_x)\n",
    "            probs = torch.sigmoid(y_pred_logits)\n",
    "            preds = (probs > 0.5).float().flatten()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            all_true.extend(batch_y)\n",
    "\n",
    "    all_preds = torch.tensor(all_preds)\n",
    "    all_probs = torch.tensor(all_probs)\n",
    "    all_true = torch.tensor(all_true)\n",
    "\n",
    "    acc = accuracy(all_probs, all_true)\n",
    "    prec = precision(all_probs, all_true)\n",
    "    rec = recall(all_probs, all_true)\n",
    "    f1 = f1_score(all_probs, all_true)\n",
    "    auc = roc_auc(all_probs, all_true)\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print_confusion_matrix(torch.tensor(all_probs), torch.tensor(all_true))\n",
    "        log_epoch(epoch, avg_loss, accuracy=avg_acc)\n",
    "\n",
    "print(f\"Epoch {epoch} | Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f622b-d9c4-4e78-be3d-d09527034231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
