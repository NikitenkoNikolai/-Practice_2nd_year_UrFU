{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735189e3-3833-4034-a04b-241d6120753f",
   "metadata": {},
   "source": [
    "# 2.1 Кастомный Dataset класс "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e7d2c52e-c405-43ed-b652-50a911b1cf26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#без знания pandas и возможностей sklearn я не знаю как решить задачу\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path, target_col, numerical_cols=None, categorical_cols=None, transform=True,\n",
    "                sep = None):\n",
    "        \"\"\"\n",
    "        :param file_path: путь к .csv файлу\n",
    "        :param target_col: имя целевой переменной\n",
    "        :param numerical_cols: список числовых признаков\n",
    "        :param categorical_cols: список категориальных признаков\n",
    "        :param transform: применять ли нормализацию и кодирование\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(file_path, sep=sep)\n",
    "        \n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Разделение признаков и целевой переменной\n",
    "        self.features = self.data.drop(columns=[target_col])\n",
    "        self.targets = self.data[target_col].values\n",
    "        \n",
    "        # Автоматическое определение типов признаков\n",
    "        if numerical_cols is None and categorical_cols is None:\n",
    "            numerical_cols = []\n",
    "            categorical_cols = []\n",
    "            for col in self.features.columns:\n",
    "                if self.features[col].dtype == 'O' or len(self.features[col].unique()) < 20:\n",
    "                    categorical_cols.append(col)\n",
    "                else:\n",
    "                    numerical_cols.append(col)\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        # Преобразование данных\n",
    "        self.transform = transform\n",
    "        if transform:\n",
    "            self._apply_transform()\n",
    "\n",
    "    def _apply_transform(self):\n",
    "        \"\"\"\n",
    "        Применяет нормализацию и one-hot кодирование\n",
    "        \"\"\"\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), self.numerical_cols),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), self.categorical_cols)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        X_transformed = preprocessor.fit_transform(self.features)\n",
    "        # Если матрица разреженная — переводим в плотный формат\n",
    "        if hasattr(X_transformed, \"toarray\"):\n",
    "            X_transformed = X_transformed.toarray()\n",
    "\n",
    "        X_transformed = np.nan_to_num(X_transformed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        self.X_tensor = torch.tensor(X_transformed, dtype=torch.float32)\n",
    "        self.y_tensor = torch.tensor(self.targets, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_tensor[idx], self.y_tensor[idx]\n",
    "        \n",
    "def mse(y_pred: torch.Tensor, y_true:torch.Tensor) -> torch.Tensor:\n",
    "    return ((y_pred - y_true)**2).mean()\n",
    "        \n",
    "def log_epoch(epoch, avg_loss, **metrics):\n",
    "    message = f'Epoch: {epoch}\\t loss:{avg_loss}'\n",
    "    for k, v in metrics.items():\n",
    "        message += f'\\t{k}: {v:.4f}'\n",
    "    print(message)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_bin = (y_pred>0.5).float()\n",
    "    return (y_pred_bin == y_true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a8bdd-9a4c-4e1b-a2dc-a1e49c96a9c9",
   "metadata": {},
   "source": [
    "# 2.2 Эксперименты с различными датасетами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a8c9773c-b926-4840-83d3-1a54acb66522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "98e317ea-749d-433d-9555-8b3fb2c04b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, in_features, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear.forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0daae14a-4a40-4702-9d00-049cf9439f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5672\\886287871.py:20: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  self.data = pd.read_csv(file_path, sep=sep)\n"
     ]
    }
   ],
   "source": [
    "url_linear = 'https://raw.githubusercontent.com/thainazanfolin/min_sleep_efficiency/refs/heads/main/Sleep_Efficiency_table.csv'\n",
    "import kagglehub\n",
    "\n",
    "url_logict = kagglehub.dataset_download(\"yasserh/titanic-dataset\") #Взял все датасеты из своих наработок\n",
    "#выбираем колонку по которой будем предиктить\n",
    "# print(pd.read_csv(url_linear, sep=';'))\n",
    "reg_dataset = CustomDataset(url_linear, target_col='Sleep efficiency', sep=';')\n",
    "\n",
    "clf_dataset = CustomDataset(url_logict+'\\\\Titanic-Dataset.csv', target_col='Survived')\n",
    "\n",
    "train_size_reg = int(0.8 * len(reg_dataset))\n",
    "val_size_reg = len(reg_dataset) - train_size_reg\n",
    "train_reg, val_reg = random_split(reg_dataset, [train_size_reg, val_size_reg])\n",
    "\n",
    "train_loader_reg = DataLoader(train_reg, batch_size=32, shuffle=True)\n",
    "val_loader_reg = DataLoader(val_reg, batch_size=32)\n",
    "\n",
    "# То же самое для классификации\n",
    "train_size_clf = int(0.8 * len(clf_dataset))\n",
    "val_size_clf = len(clf_dataset) - train_size_clf\n",
    "train_clf, val_clf = random_split(clf_dataset, [train_size_clf, val_size_clf])\n",
    "\n",
    "train_loader_clf = DataLoader(train_clf, batch_size=32, shuffle=True)\n",
    "val_loader_clf = DataLoader(val_clf, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e1a0f9aa-76f1-46a8-8d04-99d2fc9e1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\t loss:1.9074078928555633e+24\n",
      "Epoch: 20\t loss:inf\n",
      "Epoch: 30\t loss:inf\n",
      "Epoch: 40\t loss:nan\n",
      "Early stopping на эпохе - 40\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "# Решил разделить датасет на тренировочную и тестовую выборки, так как по определению early_stop ориентируется на ошибку \n",
    "# валидационной выборки\n",
    "train_size = int(0.8 * len(reg_dataset)) # train\n",
    "val_size = len(reg_dataset) - train_size # validate\n",
    "train_dataset, val_dataset = random_split(reg_dataset, [train_size, val_size])\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size = 128,\n",
    "#     shuffle = True,\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "L1_coefficient = 0.01 # Добавили коэфициент влияния штрафа на Лассо\n",
    "L2_coefficient = 0.01 # Добавили коэфициент влияния штрафа на Ridge \n",
    "Lasso = 0\n",
    "Ridge = 0\n",
    "\n",
    "early_stop = EPOCHS*0.4 # этот параметр нужен, чтобы иметь лимит при обучении, в случае увеличения эпох, качество ошибки не меняется или ниже\n",
    "no_improvement = 0\n",
    "best_val_loss = 1e-9\n",
    "delta_number = 0.0001 # Добавил, чтобы модель учитывала не все улучшения, а только значимые. А то 1^(-10) улучшение как то несерьезно\n",
    "\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 100\n",
    "model = LinearRegressionTorch(reg_dataset.X_tensor.shape[1])\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(batch_x)\n",
    "        \n",
    "        \n",
    "        if L1_coefficient != 0 or L2_coefficient != 0:    \n",
    "            Lasso = 0\n",
    "            Ridge = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'bias' not in name: # в формуле Лассо и Ridge мы суммируем только веса\n",
    "                    # Ошибка по формуле sum(|wi|) * coef, а после param.abs ставим .sum, так как param - тензор\n",
    "                    Lasso += param.abs().sum() \n",
    "                    # Ridge это то же Лассо, только веса беруться в квадрате\n",
    "                    Ridge += (param ** 2).sum()\n",
    "                    \n",
    "        '''\n",
    "        Сделал универсальный способ учета ошибки.\n",
    "        Теперь это и LassoRegression в случае если будет указан только L1, и RidgeRegression если указывать только L2, \n",
    "        и ElasticNetRegression если указан и L1 и L2, и обычная линейная регрессия если ничего не указывать \n",
    "        \n",
    "        Все по формуле ElasticNet = MSE + sum(|wi|) * coef_l1 + sum(wi^2) * coef_l2 \n",
    "        '''\n",
    "        loss = loss_fn.forward(y_pred, batch_y) + L1_coefficient * Lasso + L2_coefficient * Ridge\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader) #ошибка на обучающей выборке \n",
    "\n",
    "    model.eval() #переключаем модель в режим оценки \n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            y_pred = model(batch_x)\n",
    "            loss = loss_fn(y_pred, batch_y) # Здесь штрафы не нужны, так как мы проверям качество модели в чистом виде\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    if avg_val_loss < best_val_loss-delta_number:\n",
    "        no_improvement = 0\n",
    "        best_val_loss = avg_val_loss\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        log_epoch(epoch, avg_val_loss)\n",
    " \n",
    "    if early_stop <= no_improvement:\n",
    "        print(f\"Early stopping на эпохе - {epoch}\")\n",
    "        break\n",
    "    # print(model.linear.weight.data, model.linear.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ac468b3d-b097-466b-8fa2-51c877a79aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Сделаю только для бинарной классификации, сложно как то для 1.5 часа решения домашки\n",
    "def accuracy(y_pred, y_true): #вынес в отдельную функцию\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float()\n",
    "    return (y_pred == y_true).float().mean().item()\n",
    "\n",
    "def precision(y_pred, y_true):\n",
    "    y_pred = (y_pred.sigmoid() > 0.5).float()#Применяем sigmoid, чтобы получить вероятности от 0 до 1\n",
    "    #это случаи (tp), когда модель сказала да (y_pred == 1) и на самом деле - да (y_true == 1) \n",
    "    tp = (y_pred * y_true).sum().item()\n",
    "    #это случаи (fp), когда модель сказала да (y_pred == 1), а на самом деле - нет (y_true == 0).\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum().item()\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y_pred, y_true):\n",
    "    y_pred = (y_pred.sigmoid() > 0.5).float()\n",
    "    tp = (y_pred * y_true).sum().item()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum().item()\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(y_pred, y_true):\n",
    "    p = precision(y_pred, y_true)\n",
    "    r = recall(y_pred, y_true)\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2*p*r / (p + r) #изначально по формуле 2/(1/p + 1/r), но я раскрыл скобки просто\n",
    "\n",
    "def roc_auc(y_pred, y_true):\n",
    "    y_scores = y_pred.sigmoid().detach().numpy()#выход модели до sigmoid\n",
    "    y_true = y_true.numpy()#реальные метки [0, 1]\n",
    "    \n",
    "    #cортировка по уверенности модели\n",
    "    #мы делаем список пар (score, true_label) и сортируем его по убыванию уверенности модели.\n",
    "    y_scores = y_scores.reshape(-1)  #делаем 1d (на случай, если пришёл 2d)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    scores = np.column_stack((y_scores, y_true))  # связываем предсказания и правду\n",
    "    scores = scores[scores[:, 0].argsort()][::-1]  # сортировка по убыванию уверенности\n",
    "    \n",
    "    #Общее число положительных и отрицательных примеров\n",
    "    total_pos = y_true.sum()\n",
    "    total_neg = len(y_true) - total_pos\n",
    "\n",
    "    tp = fp = 0\n",
    "    tpr_list = [0]\n",
    "    fpr_list = [0]\n",
    "\n",
    "    for score in scores:\n",
    "        if score[1] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr = tp / total_pos\n",
    "        fpr = fp / total_neg\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    #Вычисление площади под кривой (AUC)\n",
    "    auc = 0\n",
    "    for i in range(1, len(tpr_list)):\n",
    "        #Интегрирование методом трапеций\n",
    "        auc += (fpr_list[i] - fpr_list[i - 1]) * (tpr_list[i] + tpr_list[i - 1]) / 2\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y_pred, y_true):\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float().numpy().flatten()# Преобразуем в numpy для удобства\n",
    "    y_true = y_true.numpy().flatten()\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    # Выводим confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"               | Predicted Pos | Predicted Neg |\")\n",
    "    print(\"---------------|--------------|--------------|\")\n",
    "    print(f\"Actual Pos     |     {tp:4d}    |     {fn:4d}    |\")\n",
    "    print(f\"Actual Neg     |     {fp:4d}    |     {tn:4d}    |\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Считаем метрики\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Выводим метрики\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall   : {recall:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02bb6b59-68cd-4efc-bb94-6a6f48b25a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 классы\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5672\\1616586711.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print_confusion_matrix(torch.tensor(all_probs), torch.tensor(all_true))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 10\t loss:0.40193198521931967\taccuracy: 0.8278\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 20\t loss:0.3782078969809744\taccuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 30\t loss:0.355729270974795\taccuracy: 0.8625\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 40\t loss:0.33801709844006433\taccuracy: 0.8639\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 50\t loss:0.3283705747789807\taccuracy: 0.8778\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 60\t loss:0.3114394168059031\taccuracy: 0.8889\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 70\t loss:0.29436913728713987\taccuracy: 0.8917\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 80\t loss:0.2846115903721915\taccuracy: 0.8986\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 90\t loss:0.2684530324406094\taccuracy: 0.9097\n",
      "Confusion Matrix:\n",
      "               | Predicted Pos | Predicted Neg |\n",
      "---------------|--------------|--------------|\n",
      "Actual Pos     |       78    |        0    |\n",
      "Actual Neg     |      101    |        0    |\n",
      "\n",
      "\n",
      "Accuracy : 0.4358\n",
      "Precision: 0.4358\n",
      "Recall   : 1.0000\n",
      "F1 Score : 0.6070\n",
      "Epoch: 100\t loss:0.25843607551521725\taccuracy: 0.9139\n",
      "Epoch 100 | Loss: nan | Acc: 0.4358, Prec: 0.4358, Rec: 1.0000, F1: 0.6070, AUC: 0.8247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 100\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "\n",
    "n_classes = 2# количество разнообразных классов\n",
    "print(n_classes,'классы')\n",
    "\n",
    "if n_classes > 2:\n",
    "    model = LogisticRegressionTorch(X.shape[1], n_classes)# выбираем модель с несколькими классами\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() # Взял функцию для высчитывания ошибки в случае, если берем многоклассовый датасет\n",
    "    '''\n",
    "    С многоклассовым датасетом лучше работает Adam оптимизатор, \n",
    "    чем с SGD (SGD тоже работает нормально, но может застрять в локальных минимумах)\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "else:\n",
    "    model = LogisticRegressionTorch(clf_dataset.X_tensor.shape[1])# выбираем модель с несколькими классами\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "# Разделение на train/val\n",
    "train_size = int(0.8 * len(clf_dataset))\n",
    "val_size = len(clf_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(clf_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()#включаем режим обучения\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_prec = 0\n",
    "    total_rec = 0\n",
    "    total_f1 = 0\n",
    "    total_auc = 0\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(batch_x)\n",
    "        loss = loss_fn.forward(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if n_classes > 2:\n",
    "            preds = y_pred.argmax(dim=1)#Для accuracy сравниваем argmax предсказаний и реальных меток\n",
    "            total_acc += (preds == batch_y).float().mean().item()\n",
    "        else:\n",
    "            probs = torch.sigmoid(y_pred)\n",
    "            preds = (probs > 0.5).float()\n",
    "            total_acc += (preds == batch_y).float().mean().item()\n",
    "            \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc/ len(train_loader)\n",
    "    \n",
    "    model.eval()#включаем для обучения\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_true = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            y_pred_logits = model(batch_x)\n",
    "            probs = torch.sigmoid(y_pred_logits)\n",
    "            preds = (probs > 0.5).float().flatten()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            all_true.extend(batch_y)\n",
    "\n",
    "    all_preds = torch.tensor(all_preds)\n",
    "    all_probs = torch.tensor(all_probs)\n",
    "    all_true = torch.tensor(all_true)\n",
    "\n",
    "    acc = accuracy(all_probs, all_true)\n",
    "    prec = precision(all_probs, all_true)\n",
    "    rec = recall(all_probs, all_true)\n",
    "    f1 = f1_score(all_probs, all_true)\n",
    "    auc = roc_auc(all_probs, all_true)\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print_confusion_matrix(torch.tensor(all_probs), torch.tensor(all_true))\n",
    "        log_epoch(epoch, avg_loss, accuracy=avg_acc)\n",
    "\n",
    "print(f\"Epoch {epoch} | Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767da36e-57b0-45c4-8205-6f6501f22d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
