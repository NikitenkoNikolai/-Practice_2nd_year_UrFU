## Импорт необходимых библиотек

```python
import torch
import numpy
import time
```

## Задание 1: Создание и манипуляции с тензорами

### 1.1 Создание тензоров

```python
# Тензор 3x4 со случайными числами от 0 до 1
tensor_1 = torch.rand((3, 4))

# Тензор 2x3x4, заполненный нулями
tensor_2 = torch.zeros((2, 3, 4))

# Тензор 5x5, заполненный единицами
tensor_3 = torch.ones((5, 5))

# Тензор 4x4 из чисел 0–15
tensor_4 = torch.arange(0, 16).reshape(4, 4)

lst_tensor = [tensor_1, tensor_2, tensor_3, tensor_4]
for i in range(len(lst_tensor)):
    print(f'Вывод {i+1}')
    print(lst_tensor[i])
```

#### Вывод:
```
Вывод 1
tensor([[0.2395, 0.7870, 0.7401, 0.7896],
        [0.5734, 0.2338, 0.3128, 0.9662],
        [0.0896, 0.8370, 0.7336, 0.1947]])

Вывод 2
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],
        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])

Вывод 3
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])

Вывод 4
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])
```

### 1.2 Операции с тензорами

```python
tensor_A = torch.rand((3, 4))
tensor_B = torch.rand((4, 3))

print(f'Тензор А \n{tensor_A}')
print(f'Тензор B \n{tensor_B}')

tensor_A_transpose = tensor_A.transpose(0, 1)
print(f'Транспонирование тензора A \n{tensor_A_transpose}')

multi_AB = tensor_A.mm(tensor_B)
print(f'Матричное умножение A и B \n{multi_AB}')

Btranspose = tensor_B.transpose(0, 1)
multi_A_Btranspose = tensor_A * Btranspose
print(f'транспонированный B \n{Btranspose}')
print(f'Поэлементное умножение A и транспонированного B \n{multi_A_Btranspose}')

sum_A = torch.sum(tensor_A)
print(f'Вычислите сумму всех элементов тензора A \n{sum_A}')
```

#### Вывод:
```
Тензор А 
tensor([[0.9666, 0.3297, 0.4372, 0.7688],
        [0.3517, 0.1627, 0.2282, 0.8802],
        [0.0316, 0.7811, 0.2767, 0.2779]])

Тензор B 
tensor([[0.3806, 0.9090, 0.7039],
        [0.3294, 0.5256, 0.4356],
        [0.2007, 0.6260, 0.9640],
        [0.2518, 0.8301, 0.7948]])

Транспонирование тензора A 
tensor([[0.9666, 0.3517, 0.0316],
        [0.3297, 0.1627, 0.7811],
        [0.4372, 0.2282, 0.2767],
        [0.7688, 0.8802, 0.2779]])

Матричное умножение A и B 
tensor([[0.7578, 1.9638, 1.8565],
        [0.4549, 1.2787, 1.2379],
        [0.3949, 0.8432, 0.8501]])

транспонированный B 
tensor([[0.3806, 0.3294, 0.2007, 0.2518],
        [0.9090, 0.5256, 0.6260, 0.8301],
        [0.7039, 0.4356, 0.9640, 0.7948]])

Поэлементное умножение A и транспонированного B 
tensor([[0.3679, 0.1086, 0.0877, 0.1936],
        [0.3197, 0.0855, 0.1428, 0.7306],
        [0.0223, 0.3402, 0.2667, 0.2209]])

Вычислите сумму всех элементов тензора A 
tensor(5.4923)
```

### 1.3 Индексация и срезы

```python
tensor555 = torch.randint(0, 10, (5, 5, 5))
print(f'Изначальный тезор \n{tensor555}')

print(f'Все первые строки \n{tensor555[:, 0]}')
print(f'Первая строка 1-го слоя\n{tensor555[0, 0]}')

print(f'Последний столбец \n{tensor555[:, :, -1]}')
print(f'Подматрица размером 2x2 из центра тензора \n{tensor555[:, 1:4, 1:4]}')
print(f'Все элементы с четными индексами \n{tensor555[::2, ::2, ::2]}')
```

#### Вывод:
```
Изначальный тезор 
tensor([[[7, 6, 4, 4, 8],
         [4, 6, 4, 8, 0],
         [3, 5, 2, 7, 4],
         [5, 2, 4, 9, 2],
         [8, 0, 5, 4, 6]],
        [[7, 0, 4, 7, 5],
         [4, 5, 0, 2, 1],
         [3, 2, 6, 7, 5],
         [6, 3, 6, 5, 7],
         [1, 4, 2, 8, 4]],
        [[8, 8, 0, 2, 1],
         [9, 4, 3, 9, 1],
         [4, 4, 3, 6, 5],
         [8, 0, 7, 3, 3],
         [6, 5, 7, 6, 2]],
        [[5, 6, 8, 9, 6],
         [5, 8, 2, 2, 7],
         [3, 3, 5, 2, 9],
         [5, 9, 2, 3, 0],
         [2, 5, 7, 5, 7]],
        [[0, 3, 6, 8, 9],
         [7, 6, 5, 6, 1],
         [2, 6, 1, 3, 0],
         [1, 2, 9, 7, 3],
         [9, 0, 8, 4, 9]]])

Все первые строки 
tensor([[7, 6, 4, 4, 8],
        [7, 0, 4, 7, 5],
        [8, 8, 0, 2, 1],
        [5, 6, 8, 9, 6],
        [0, 3, 6, 8, 9]])

Первая строка 1-го слоя
tensor([7, 6, 4, 4, 8])

Последний столбец 
tensor([[8, 0, 4, 2, 6],
        [5, 1, 5, 7, 4],
        [1, 1, 5, 3, 2],
        [6, 7, 9, 0, 7],
        [9, 1, 0, 3, 9]])

Подматрицу размером 2x2 из центра тензора 
tensor([[[6, 4, 8],
         [5, 2, 7],
         [2, 4, 9]],
        [[5, 0, 2],
         [2, 6, 7],
         [3, 6, 5]],
        [[4, 3, 9],
         [4, 3, 6],
         [0, 7, 3]],
        [[8, 2, 2],
         [3, 5, 2],
         [9, 2, 3]],
        [[6, 5, 6],
         [6, 1, 3],
         [2, 9, 7]])

Все элементы с четными индексами 
tensor([[[7, 4, 8],
         [3, 2, 4],
         [8, 5, 6]],
        [[8, 0, 1],
         [4, 3, 5],
         [6, 7, 2]],
        [[0, 6, 9],
         [2, 1, 0],
         [9, 8, 9]])
```

### 1.4 Работа с формами

```python
tensor24 = torch.arange(0, 24)
print(f'Тензор размером 2x12 \n{tensor24.reshape((2,12))}')
print(f'Тензор размером 3x8 \n{tensor24.reshape((3,8))}')
print(f'Тензор размером 4x6 \n{tensor24.reshape((4,6))}')
print(f'Тензор размером 2x3x4 \n{tensor24.reshape((2,3,4))}')
print(f'Тензор размером 2x2x2x3 \n{tensor24.reshape((2,2,2,3))}')
```

#### Вывод:
```
Тензор размером 2x12 
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])

Тензор размером 3x8 
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],
        [ 8,  9, 10, 11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20, 21, 22, 23]])

Тензор размером 4x6 
tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17],
        [18, 19, 20, 21, 22, 23]])

Тензор размером 2x3x4 
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],
        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]])

Тензор размером 2x2x2x3 
tensor([[[[ 0,  1,  2],
          [ 3,  4,  5]],
         [[ 6,  7,  8],
          [ 9, 10, 11]]],
        [[[12, 13, 14],
          [15, 16, 17]],
         [[18, 19, 20],
          [21, 22, 23]]])
```

## Задание 2: Автоматическое дифференцирование

### 2.1 Простые вычисления с градиентами

```python
x = torch.tensor([0.0, 1.0, 2.0], requires_grad=True)
y = torch.tensor([3.0, 4.0, 5.0], requires_grad=True)
z = torch.tensor([6.0, 7.0, 8.0], requires_grad=True)

function = x**2 + y**2 + z**2 + 2*x*y*z
loss = function.sum()
loss.backward()

print(f'производная по x = {x.grad}')
print(f'производная по y = {y.grad}')
print(f'производная по z = {z.grad}')
```

#### Вывод:
```
производная по x = tensor([36., 58., 84.])
производная по y = tensor([ 6., 22., 42.])
производная по z = tensor([12., 22., 36.])
```

### 2.2 Градиент функции потерь

```python
x = torch.tensor([1.0, 2.0, 3.0])
y_true = torch.tensor([3.0, 5.0, 7.0]) # y = 2x + 1
w = torch.tensor(0.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

y_pred = w * x + b
mse = ((y_pred - y_true) ** 2).mean()
mse.backward()

print(f'mse = {mse.item()}')
print(f'градиент w = {w.grad.item()}')
print(f'градиент b = {b.grad.item()}")
```

#### Вывод:
```
mse = 27.66666603088379
градиент w = -22.666667938232422
градиент b = -10.0
```

### 2.3 Цепное правило

```python
x = torch.tensor(2.0, requires_grad=True)
function = torch.sin(x**2 + 1)
function.backward(retain_graph=True)

print(f'df/dx = {x.grad}')
df_dx = torch.autograd.grad(function, x)[0]
print(f'Проверка с помощью torch.autograd.grad = {df_dx}')
```

#### Вывод:
```
df/dx = 1.1346487998962402
Проверка с помощью torch.autograd.grad = 1.1346487998962402
```

## Задание 3: Сравнение производительности CPU vs CUDA

### 3.1 Подготовка данных

```python
tensor1 = torch.rand((64, 1024, 1024))
tensor2 = torch.rand((128, 512, 512))
tensor3 = torch.rand((256, 256, 256))

print(f"Tensor 64x1024x1024: {tensor1.shape}")
print(f"Tensor 128x512x512: {tensor2.shape}")
print(f"Tensor 256x256x256: {tensor3.shape}")
```

#### Вывод:
```
Tensor 64x1024x1024: torch.Size([64, 1024, 1024])
Tensor 128x512x512: torch.Size([128, 512, 512])
Tensor 256x256x256: torch.Size([256, 256, 256])
```

### 3.2 Функция измерения времени

```python
def measure_time(fn, device='cpu', num_runs=10):
    if device == 'cuda': 
        if not torch.cuda.is_available():
            print("CUDA не доступна, используется CPU") 
            device = 'cpu'

    if device == 'cuda':
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        total_time_ms = 0
        for _ in range(num_runs):
            start_event.record()
            fn()
            end_event.record()
            torch.cuda.synchronize()
            total_time_ms += start_event.elapsed_time(end_event)
        avg_time = total_time_ms / num_runs
        print(f"GPU Среднее время: {avg_time:.2f} мс")
    else:
        total_time_s = 0
        for _ in range(num_runs):
            start = time.time()
            fn()
            end = time.time()
            total_time_s += (end - start)
        avg_time = total_time_s * 1000 / num_runs
        print(f"CPU Среднее время: {avg_time:.2f} мс")
    return avg_time
```

### 3.3 Сравнение операций

```python
def run_test(name, op):
    cpu_time = measure_time(lambda: op(tensor_a.cpu(), tensor_b.cpu()), device='cpu')
    gpu_time = float('nan')
    speedup = float('nan')
    
    if torch.cuda.is_available():
        op_cuda = lambda: op(tensor_a.cuda(), tensor_b.cuda())
        gpu_time = measure_time(op_cuda, device='cuda')
        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')
    return (name, cpu_time, gpu_time, speedup)


def benchmark_operations(tensor_a, tensor_b):
    results = []
    mm = run_test("Матричное умножение", lambda a, b: torch.bmm(a, b.transpose(1, 2)))
    plus = run_test("Сложение", lambda a, b: a + b)
    el_mm = run_test("Поэлементное умножение", lambda a, b: a * b)
    transp = run_test("Транспонирование", lambda a, _: a.transpose(1, 2))
    sumi = run_test("Сумма элементов", lambda a, _: a.sum())

    results = [mm, plus, transp, sumi]

    print("\nРезультаты:")
    print("{:<25} | {:>8} | {:>8} | {:>10}".format("Операция", "CPU (мс)", "GPU (мс)", "Ускорение"))
    print("-" * 65)

    for name, cpu_t, gpu_t, speedup in results:
        gpu_str = f"{gpu_t:.2f}" if not torch.isnan(torch.tensor(gpu_t)) else "—"
        speedup_str = f"{speedup:.1f}x" if not torch.isnan(torch.tensor(speedup)) else "—"
        print(f"{name:<25} | {cpu_t:>8.2f} | {gpu_str:>8} | {speedup_str:>10}")
```

```python
tensor_a = tensor3
tensor_b = tensor3
benchmark_operations(tensor_a, tensor_b)
```

#### Вывод:
```
GPU Среднее время: 115.68 мс
GPU Среднее время: 107.50 мс
GPU Среднее время: 110.12 мс
GPU Среднее время: 114.39 мс

Результаты:
Операция                  | CPU (мс) | GPU (мс) | Ускорение
-----------------------------------------------------------------
Матричное умножение       |   156.49 |   115.68 |      1.4x
Сложение                  |    14.69 |   107.50 |      0.1x
Поэлементное умножение     |     0.12 |   110.12 |      0.0x
Сумма элементов           |     5.37 |   114.39 |      0.0x
```

## Анализ результатов

### Какие операции получают наибольшее ускорение на GPU?
Наибольшее ускорение наблюдается у матричного умножения — это связано с тем, что GPU оптимизирован для параллельных вычислений.

### Почему некоторые операции могут быть медленнее на GPU?
Простые операции, такие как сложение и транспонирование, требуют мало вычислений, но много подготовки на GPU — основное время уходит на запуск ядра и работу с памятью.

### Как размер матриц влияет на ускорение?
Чем больше данные — тем выше ускорение на GPU. На маленьких данных оверхед на передачу между CPU и GPU может превысить выгоду от параллелизма.

### Что происходит при передаче данных между CPU и GPU?
Копирование данных занимает время. Частые обмены сильно замедляют работу, поэтому лучше минимизировать их количество.

### Комментарии
За оформление заранее простите, наверное на следующей лабораторной меньше комментариев буду писать.
